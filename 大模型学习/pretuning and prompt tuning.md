#### **Pre-trainedFine-tuning**的问题：

在大多数的下游任务微调时， **下游任务的目标与预训练的目标差距过大** 导致提升效果不明显， **微调过程中依赖大量的监督语料** 等。

#### Prompt-Tuning的动机旨在解决目前传统Fine-tuning的两个痛点问题：

**降低语义差异（Bridge the gap between Pre-training and Fine-tuning）** ：预训练任务主要以Masked Language Modeling（MLM）为主，而下游任务则重新引入新的训练参数，因此两个阶段的目标通常有较大差异。因此需要解决如何缩小Pre-training和Fine-tuning两个阶段目标差距过大的问题；

 **避免过拟合（Overfitting of the head）** ：由于在Fine-tuning阶段需要新引入额外的参数以适配相应的任务需要，因此在样本数量有限的情况容易发生过拟合，降低了模型的泛化能力。因此需要面对预训练语言模型的过拟合问题。



作者：kaiyuan
链接：https://zhuanlan.zhihu.com/p/618871247
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。





**Instruction-tuning、Fine-tuning、Prompt-Tuning的区别在哪？**

Fine-tuning：先在大规模语料上进行预训练，然后再在某个下游任务上进行微调，如BERT、T5；
Prompt-tuning：先选择某个通用的大规模预训练模型，然后为具体的任务生成一个prompt模板以适应大模型进行微调，如GPT-3；
Instruction-tuning：仍然在预训练语言模型的基础上，先在多个已知任务上进行微调（通过自然语言的形式），然后再推理某个新任务上进行zero-shot。

https://blog.csdn.net/qq_39388410/article/details/128265846



chatGPT的三阶段：pt sft rhlf

得益于第一阶段预训练中的next token preadict任务，第一阶段完成后LLM具有广泛的知识储备和强大的续写能力。但此时LLM基本上只会续写，为了进一步提高LLM在未见任务上的指令泛化能力，即Zero-Shot能力，需要在指令数据上微调预训练模型，这一步就叫sft，有监督的微调，指令调整不仅能显著提高模型的性能和泛化能力，还能使模型生成的结果与人类交互模式更加一致。然后后面的第三阶段rhlf主要就是利用强化学习，和人类喜好对齐，提高生成内容的一致性。

sft阶段会收集大量的 {指令，回复} 数据对，得到一个指令跟随数据集。然后用指令数据集通过有监督的方式对前面训练得到的LLM进行指令调优，得到SFT模型。到这一步得到的SFT模型已经能实现很好的指令跟随。
通过在输入中添加指令，使得模型可以将指令作为上下文信息，可以引导模型生成特定任务所需的输出。这种方法可以让模型获得听懂指令的能力，可以适应于各种不同任务。

在sft阶段，重要的就是数据集，数据集长什么样子，从哪里来，质量如何，多元性如何，数量如何，如何评判数据集的质量，都是急需我们回答的问题。





FLAN模型
