## 1 LLMs 复读机问题



### 1.1 什么是 LLMs 复读机问题？

LLMs 复读机问题 就是 LLMs 出现 重复输出：

eg：ABCABCABC不断循环输出到max length

### 1.2 为什么会出现 LLMs 复读机问题？

出现 LLMs 复读机问题 的 原因：

prompt部分通常很长，在生成文本时可以近似看作不变，那么条件概率 P(B|A)也不变，一直是最大的。

生成重复内容，是语言模型本身的一个弱点，无论是否微调，都有可能出现。并且，理论上良好的指令微调能够缓解大语言模型生成重复内容的问题。但是因为指令微调策略的问题，在实践中经常出现指令微调后复读机问题加重的情况。

### 1.3 如何缓解 LLMs 复读机问题？

1. 方法一：解码方式里增加不确定性。既然容易复读那我们就增加随机性，开启do_sample选项，调高temperature；
2. 方法二：加重复惩罚。如果学的太烂，do_sample也不顶用呢？加重复惩罚，设置repetition_penalty，注意别设置太大了。不然你会发现连标点符号都不会输出了。

## 2 llama 系列问题

### 2.1 llama 输入句子长度理论上可以无限长吗？

限制在训练数据。理论上rope的llama可以处理无限长度，但问题是太长了效果不好啊，没训练过的长度效果通常不好。而想办法让没训练过的长度效果好，这个问题就叫做“长度外推性”问题。

所以接受2k的长度限制吧。

## 3 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？

Bert 的模型由多层双向的Transformer编码器组成，由12层组成，768隐藏单元，12个head，总参数量110M，约1.15亿参数量。NLU（自然语言理解）任务效果很好，单卡GPU可以部署，速度快，V100GPU下1秒能处理2千条以上。

ChatGLM-6B, LLaMA-7B模型分别是60亿参数量和70亿参数量的大模型，基本可以处理所有NLP任务，效果好，但大模型部署成本高，需要大显存的GPU，并且预测速度慢，V100都需要1秒一条。

所以建议：

1）NLU相关的任务，用BERT模型能处理的很好，如实体识别、信息抽取、文本分类，没必要上大模型；

2）NLG任务，纯中文任务，用ChatGLM-6B，需要处理中英文任务，用chinese-alpaca-plus-7b-hf

## 4 各个专业领域是否需要各自的大模型来服务？

是，各行各业的大模型是趋势。

## 5 如何让大模型处理更长的文本？

1. 动机：目前绝大多数大模型支持的token最大长度为2048，因为序列长度直接影响Attention的计算复杂度，太长了会影响训练速度。
2. 让大模型处理更长的文本 方法
3. LongChat

就两步：

step1：将新的长度压缩到原来2048长度上，这样的好处是能复用原来的位置信息，增加长度并没有破坏position的权重。

比如从2048扩展到16384，长度变为原来的8倍，那么值为10000的position_id，被压缩成10000/8=1250

代码只需要改一行：

\# 将position_ids按比例缩一下。

query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids / ratio). 

详细参考：https://kaiokendev.github.io/context

step2：用训练Vicuna的对话语料做微调，超过16k的文本被截断。

1. position等比例缩放既然有用，那后续会不会有一种新的position构造的方式，无论多长都可以归一到同样的尺度下，只要position的相对位置保持不变就可以？其实ALiBi的方法就是一个比较简单优雅的方式，可以部分解决扩展长度的问题。
2. 商业模型比如ChatGPT和Claude到底是怎么做的？这个目前都没有公开。首先语料是不缺的，所以只能是结构上的变化。但是这两个商业模型规模都是100B这个量级的，这么大的参数，如果只增加序列长度而不做其他优化的话，很难训练起来。目前有证据的方法如下：
   1. 稀疏化，GPT3的论文中曾提到有这方面的尝试。
   2. Google的周彦祺在一次分享中透露GPT-4用了MoE的技术(猜测是100B16E)，所以应该有类似的方法来保证在序列变长的情况下，仍然能高效的训练模型。
   3. Multi-Query Attention。Google的PaLM，Falcon等模型都用到过，通过权重共享来提升性能。
3. 真正的出路可能还是Linear Attention，将Attention的复杂度从 *O*(*N*2) 降低为 *O*(*N*). 比如Linear Transformer和RWKV。其实关于变长序列的问题，历史上现成的解决方案就是RNN，通过信息传递来解决。Transformer的卖点就是Attention is All your Need，丢弃了RNN，RWKV敢于把RNN拿回来，还是很有勇气，非常好的一个工作。现在的Attention就有点像历史上的MLP，每个节点之间都要建立关联，而MLP之后涌现了大量新的结构，所以Transformer是起点，后续肯定会有更合理的结构来取代它。