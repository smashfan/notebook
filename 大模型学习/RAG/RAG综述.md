写在前面
作为一个初学大模型的菜鸟，鼠鼠对于大模型可以说是一无所知，对于信息检索的了解也算是一片空白，考虑到目前对于信息检索领域的大模型中文参考资料过少，在此姑且卖弄一下阅读完英文的大模型在信息检索领域的综述整理出的一份姑且算是英译中的参考材料吧。原文为：Large Language Models for Information Retrieval: A Survey，引用自arxiv：https://arxiv.org/abs/2308.07107，肝了一万多字，关键的地方还加上了对应的论文引用，姑且立个flag，以后会定期更新读综述或者论文并进行解读，这篇文章就当是个开始之作了。

背景介绍
前面的常识性的话略过不表，其中核心的几个要点如下：

检索算法的效率至关重要
改善体验的两个角度：上游任务的查询的重新表述，下游的重新排序和读取。
上游：查询旨在细化用户查询，使其更有效的检索相关的文档。
下游：研究重新排序的方法进一步调整文章的排序。
上游任务关注效率，下游任务更关注表现。
重新排序可以满足其他特定的需求，如个性化和多样化。
经过检索和重排后，需要形成一个总结的简介文档以向用户提供。


发展路线：从布尔代数到神经模型的集成
布尔代数
IR以基于术语的方法和布尔逻辑为基础，重点关注文档检索的关键字匹配。

后续向量空间开始引入
统计语言模型引入，通过上下文和概率考虑来改进相关性估计。
BM25算法诞生，通过考虑词频率和文档长度变化，彻底改变了相关性排序。
BM算法：https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/okapi_trec3.pdf

神经网络时代
模型在捕捉复杂的上下文线索和语义细微差别方面表现出色。

缺点：数据稀缺性、可解释性以及可能产生的似是而非的响应。

总结
IR的发展仍然是一个平衡传统优势(如BM25算法的高效率)与现代神经架构带来的卓越能力(如语义理解)的过程。

四个维度的背景
Query Rewriter（查询重写器）
目标：提高用户查询的准确性和表达性。
位置与作用：该模块位于信息检索pipline的早期阶段，承担着精炼或修改初始查询以更准确地与用户的信息需求保持一致的关键作用。
查询扩展技术作为Query Rewriter的重要组成部分，代表了实现查询表达式细化的主流方法，如：伪相关反馈。
查询重写器除了可以提高一般情况下的搜索效率外，还可以应用于各种专门的检索上下文，例如个性化搜索和会话搜索，从而进一步证明了它的重要性。
Retriever(检索器)
检索器通常在IR的早期阶段用于文档recall。
经典的“词袋”模型BM25已经证明了它的鲁棒性和高效率。
随着神经IR范式的崛起，流行的方法主要围绕将查询和文档投影到高维向量空间，然后通过内积计算计算它们的相关性分数。
Reranker（重排）
主要关注检索文档集中的文档的细粒度重新排序。
重新排序模块更强调文档排序的质量。
重新器有助于采用专门的排名策略，以满足不同的用户需求，如个性化和多样化的搜索结果。
通过整合特定领域的目标，重新排名模块可以提供定制的和有目的的搜索结果，增强整体用户体验。
Reader
能够理解实时用户意图并基于检索文本生成动态响应。
阅读器模块以更直观的方式组织答案文本，模拟人类访问信息的自然方式。
为了提高生成回复的可信度，将参考文献整合到生成回复中。
查询重写器
传统方法
传统的查询重写方法是通过对初始查询检索到的排名靠前的文档进行分析，迭代地改进用户的原始查询来解决这个问题的。
这些方法通常依赖于预定义的规则或启发式，限制了它们完全捕捉用户意图的细微差别的能力。
例如：RM3, LCE, KL expansion, and relevance modeling。
重写的目标
Ad-hoc Retrieval（检索不匹配，如：手机和移动电话，网络的主要信息是手机，以移动电话为检索词难以搜到过多的有效信息）
会话搜索：基于不断发展的对话改进和调整系统的响应
Ad-hoc Retrieval
大语言模型提供的一些显著的优势
更好的语义的理解，能够更有效地捕获查询的含义和上下文。
广泛的知识，这些知识使他们能够利用他们对各种主题的理解来生成相关的和上下文合适的查询重写。
首遍检索的独立性，传统的伪相关反馈(PRF)方法检索一组伪相关文档作为源，以改进原始查询，但是这个可能会引入无关的噪声和潜在的有害信息，相比之下LLM可以直接基于原始查询生成查询重写，这与第一遍检索无关，并防止了潜在的噪声。
代表性技术成果：Query2Doc
steps：

LLM根据查询生成相关的段落，一定意义上相当于先生成原始的答案。
通过合并生成的信息来扩展原始查询。

对图片的说明：

N = 0 N=0N=0对应的是0-shot，N > 0 N>0N>0对应的是few-shot。
LLM通过使用几个提示范例生成原始查询的相关段落。
然后，将原始查询和生成的通道结合起来构造一个新的查询。
检索模块使用这个新查询来检索相关文档的列表。
Conversational Search（会话搜索）
定义：会话式搜索则涉及用户与检索系统之间的动态交互，系统通过一系列的来回交换来响应用户的查询并澄清用户的信息需求。

优势

LLM具有较强的上下文理解能力，能够更好地理解用户与系统之间多回合对话背景下的用户搜索意图。
LLM表现出强大的生成能力，允许它们模拟用户与系统之间的对话，从而促进更健壮的搜索意图建模。
代表成果：LLMCS(大型语言模型知道您的上下文搜索意图)


LLMCS提示LLM从多个角度生成查询重写和更长的假设系统响应。
然后生成的输出聚合成一个集成的表示，该表示健壮地捕获用户的完整搜索意图。
实验表明将假设的响应与简洁的查询重写结合起来，通过显式地补充更合理的搜索意图，显著提高了搜索性能。
LLMCS:https://arxiv.org/abs/2303.06573

丰富文本中包含的资源(原文使用的直接是Data resourses)
背景：查询重写方法通常需要补充语料库来丰富原始查询，尽管LLM通过其参数固有地融入了世界知识，但是对于特定的领域，这样的一般知识可能是不够的，因此需要特定于领域的语料库来提供更全面和特定于领域的信息。

Inherent Knowledge of LLMs
LLM能够在其参数中存储知识，因此很自然地选择利用这些知识进行查询重写。

相关的代表性成果有：HyDE，Query2doc。

HyDE
HyDE是基于LLM的查询重写的开创性工作，由LLM根据给定的查询生成一个假设的文档，然后使用密集检索器从语料库中检索与生成的文档相关的相关文档。
HyDE:https://arxiv.org/abs/2212.10496

Query2Doc
Query2doc通过少量演示来提示LLM生成伪文档，然后用生成的伪文档扩展查询。

此外，文献 Query expansion by prompting large language models研究了不同提示方法和不同模型大小对查询重写的影响。

为了更好地适应冷启动的检索和基于LLM的阅读器，一个小型语言模型被用作重写器，该重写器使用强化学习技术进行训练，并获得基于LLM的阅读器提供的奖励。

Inherent Knowledge of LLMs and Document Corpus
尽管LLM表现出非凡的能力，但他们对特定领域缺乏熟悉可能导致产生幻觉或不相关的查询。为此有研究提出了一种混合方法，即用文档语料库增强基于LLM的查询重写方法。

文档语料库的集成的几个显著优势
它通过对特定主题领域的查询生成过程进行微调来提供特定于领域的知识，从而支持有针对性的专用IR方法。
通过结合从语料库中提取的事实信息，确保查询在可靠和可验证的知识中。
它通过丰富当代信息的查询，超越LLM所包含的知识，并确保增强信息的丰富性和及时性，从而结合了最新的概念。
通过利用相关文档作为补充资源来优化查询生成过程，从而有效地减少不相关信息的生成，并提高上下文相关输出的产生，从而支持相关性。
实现方法（三条解决的方案）
Late fusion of LLM-only re-writing and traditional PRF retrieval results
Combining retrieved relevant documents in the prompts of LLMs
Enhancing factuality of generative relevance feedback (GRF) by pseudo relevance feedback (PRF)
Late fusion of LLM-only re-writing and traditional PRF retrieval results
字面意思，对对检索结果和LLM输出的外部语料中不存在的上下文使用加权融合方法。

参考文献：https://arxiv.org/abs/2305.07477

Combining retrieved relevant documents in the prompts of LLMs
QUILL提出了一种两阶段提取管道，将LLM中的知识转移到小模型中，以便更有效地重写查询。
参考文献： https://arxiv.org/abs/2210.15718
LameR提出了一个检索-重写-检索框架。
采用BM25作为检索器，不使用任何带注释的查询文档对
参考文献：https://arxiv.org/abs/2304.14233
InteR提出了一个框架，支持搜索引擎和LLM之间的多轮交互。
能够使用LLM生成的知识扩展查询，同时允许LLM使用搜索引擎提供的相关文档改进提示
参考文献：https://arxiv.org/abs/2305.07402
Enhancing factuality of generative relevance feedback (GRF) by pseudo relevance feedback (PRF)
背景：生成文件往往是相关的和多样化的，同时表现出幻觉的特点，传统文献通常被认为是事实信息的可靠来源。

GRM提出了一种被称为关联感知样本估计(RASE)的新技术，RASE利用从集合中检索到的相关文档为生成的文档分配权重，这样，GRM就保证了相关性反馈的多样性，同时又保持了高度的真实性。

参考文献：https://arxiv.org/abs/2306.09938
Generation Methodology
有三种主要范例用于利用LLM来完成查询重写的任务。

提示方法
知识蒸馏方法
微调方法
提示方法（有三种主要范例用于利用LLM来完成查询重写的任务）
Zero-shot prompting：指示模型生成特定主题的文本，而无需事先接触该领域或主题的训练示例。实践证明：Zero-shot prompting是一种简单而有效的查询重写方法。
Few-shot prompting：也称为上下文学习，涉及向模型提供与期望任务或领域相关的有限示例或演示集。备注：如Query2Doc。
为了进一步研究不同提示符设计的影响，有文章探讨了8种不同的提示符，如提示LLM生成查询扩展项而不是整个伪文档、CoT提示等。实验表明，该提示不如Query2Doc中有效。
Chain-of-thought prompting：是一种涉及迭代提示的策略，为模型提供一系列指令或部分输出。在会话搜索中，查询重写的过程是多回合的，这意味着查询需要随着搜索引擎和用户的交互而逐步细化，这一过程与CoT过程高度相似。注意：在ad-hoc search中，查询重写只有一轮，所以CoT只能用简单粗暴的方式来完成。
如：研究人员在指令中增加了“先给出理由再回答”。
微调方法
微调的过程通常包括使用预训练的LM，如GPT-3，并在特定于目标领域的数据集上进一步训练它
数据集可以由查询、重写和相关标签组成，也可以通过人工专业知识和数据增强技术的组合生成
在微调期间，调整模型的参数以优化其在特定领域任务上的性能
知识蒸馏方法
背景：尽管基于LLM的方法在查询重写任务方面有了显著的改进，但它们在在线部署方面的实际实现受到LLM计算需求造成的大量延迟的阻碍。

案例：QUILL的实现
使用检索增强LLM作为教授模型，使用普通LLM作为教师模型，使用轻量级BERT模型作为学生模型。
教授模型在两个广泛的数据集上进行训练，即Orcas-I和EComm，这两个数据集专门用于查询意图理解。
采用两阶段的精馏过程将知识从教授模型转移到教师模型，然后再从教师模型转移到学生模型。
结论：这种知识蒸馏方法超越了简单地将模型大小从基础扩展到XXL，从而产生了更实质性的改进。（XXL指的应该是T5家族的一个模型）
rewrite-retrieve-read framework
参考文献：https://arxiv.org/abs/2305.14283

首先使用LLM通过提示重写查询，然后使用检索增强读取过程。
将一个可训练的重写器作为一个小型语言模型实现，以进一步调整搜索查询，使其与冷启动的检索器和LLM读者的需求保持一致，从而提升框架的有效性。
重写器的精炼过程

使用伪数据进行监督热身训练。
随后，检索-读取管道被描述为一个强化学习场景，重写者的训练作为一个策略模型，以最大化管道性能奖励。
限制与缺陷
虽然候选文档有助于呈现原始查询的补充上下文，但大多数现有作品通过重复或替代策略强调原始查询的突出性。
缺乏专门用于评估查询重写的指标，现有模型严重依赖于下游检索任务来评估查询重写器的有效性，如何直接确定重写的查询是否反映了人类的意图并有效地服务于特定的任务仍然没有解决。
检索器
由于文档数量很多，同时最终的排名的质量也很重要，所以检索器需要关注以下两个维度的内容：

高查找效率
高召回率
近年来，检索模型已经从依赖统计算法转向依赖神经模型，而神经检索器的成功依赖于两个关键因素:数据和模型。从数据的角度来看，大量高质量的训练数据是必不可少的；从模型的角度来看，一个强表征的神经结构允许检索器有效地存储和应用从训练数据中获得的知识。

困境
用户查询通常很短且含糊不清，因此很难准确理解用户对检索器的搜索意图。
文档通常包含冗长的内容和大量的噪声，这给长文档的编码和为检索模型提取相关信息带来了挑战。
收集人工注释的相关标签既耗时又昂贵，这限制了检索者的知识边界和他们跨不同应用领域进行泛化的能力。
现有的模型架构，主要建立在BERT上，表现出固有的局限性，从而限制了检索器的性能潜力
大模型带来的解决维度
利用LLM生成搜索数据

利用LLM增强模型架构。

生成搜索数据
第一个视角围绕搜索数据细化方法展开，它侧重于重新表述输入查询，以精确地表示用户意图。

第二种观点涉及训练数据增强方法，它利用LLM的生成能力来扩大密集检索模型的训练数据，特别是在zero- or few-shot scenarios中。

搜索数据细化
背景：输入查询由短句或基于关键字的短语组成，这些短语可能含糊不清，并且包含多个可能的用户意图，在这种情况下，准确确定特定的用户意图至关重要。

到目前为止，本项工作主要集中在重写器模块，但是除此之外，也有人尝试使用LLM通过精炼冗长的文档来提高检索的有效性。

训练数据增强
背景：LLM的主色的文本生成方面的能力为人工标注标签提供了一个可行的解决方案。

为什么我们需要训练数据增强？
以往对神经检索模型的研究主要集中在监督学习上，即使用特定领域的标记数据来训练检索模型，然而，这种模式固有地限制了检索器对来自其他域的分布外数据的泛化能力。检索模型需要有zero-shot和few-shot的研究需求。而在没有足够标签信号的情况下，提高模型在目标域中有效性的一种常见做法是通过数据增强。

实现的简单思路
创建伪查询或基于现有集合的相关标签。



PS.相关标签生成方法不将问题视为输入，而是将它们的生成概率视为软相关标签，这取决于检索到的段落。

伪查询生成
参考实现案例：inPairs。

实现思路：

使用一组查询文档对作为参考（demonstrations）
这些查询对与文档组合在一起，并作为GPT-3的输入呈现，GPT-3随后为给定文档生成可能的相关查询。
将相同的查询对与各种文档相结合，很容易创建大量的合成训练样本，并支持在特定目标域上微调检索器。
为了提高这些合成样本的可靠性，使用了一个微调模型来过滤生成的查询，只有估计相关度得分最高的前对才会被保留下来进行训练。
这种“generating-then-filtering”范例可以以往返过滤的方式迭代地进行。
首先在生成的样本上微调检索器。
使用此检索器筛选生成的样本。
重复上述过程直至收敛
一些限制与解决
在实践中，做的成本依旧很高，于是平衡生成样例的花费和生成的样例质量成为了一个十分关键的问题。

UDAPDR和SPTAR相继给出了对应的解决方面。

解决脉络如下：

UDAPDR

首先使用LLM为目标域生成一组有限的合成查询。
这些高质量的示例随后被用作一个较小模型的提示，以生成大量查询，从而构建特定领域的训练集。
优缺点
性能显著提高
对于计算资源依旧有过高的需求
SPTAR

SPTAR仅在训练过程中优化提示的嵌入层，从而使LLM能够更好地适应生成伪查询的任务，在训练成本和生成质量之间取得良好的平衡。

相关标签的生成
背景：在部分下游任务中，收集问题也就足够了，但是存在一个问题，就是把我们收集到的问题与相关的段落联系在一起的相关性标签是有限的，从而我们可以考虑使用LLM对于标签进行生成，从而增强我们检索器的训练语料库。

参考案例：ART

先为每个问题检索相关度最高的文章。
使用LLM来生成以这些top message为条件的问题生成的概率。
经过归一化后，这些概率作为软标签用于检索器的训练
本部分总结


利用大模型增强检索器的模型架构
两个解决维度
基于编码的检索器
生成检索器
基于编码的检索器
除了数据的数量和质量外，模型的代表能力也极大地影响着检索器的有效性，受到LLM编码和理解自然语言的卓越能力的启发，一些研究人员利用LLM作为检索编码器，并研究了模型尺度对检索器性能的影响。

广义检索器（General Retriever）
由于检索器的有效性主要依赖于文本嵌入的能力，因此文本嵌入模型的发展往往对检索器的发展产生重大影响。OpenAI将相邻的文本段视为正对，便于对一组文本嵌入模型进行无监督预训练，该模型记为cpt-text，参数值从300M到175B不等，在MS MARCO和BEIR数据集上进行的实验表明，更大的模型尺度有可能在文本搜索任务的无监督学习和迁移学习中产生更好的性能。

但是不可忽略的是，从头开始训练LLM的成本令人望而却步，有工作为了克服这一限制，使用较小尺寸的t5家族模型(如Base、Large、XL和XXL)初始化双编码器的模型参数，然后在检索数据集上进行微调，实验结果再次证实，更大的模型尺寸可以带来更好的性能，特别是在零射击设置。
实验参考文献：https://arxiv.org/abs/2112.07899

任务感知检索器（Task-aware Retriever）
含义：集成特定于任务的指令，该指令包括任务的意图、域和所需检索单元的描述，如：“检索回答这个问题的维基百科文本“。

作用：这种方法可以利用强大的语言建模能力和LLM的广泛知识，在各种检索任务中精确捕获用户的搜索意图。

生成检索器
背景：传统的IR系统通常遵循“索引-检索”模式，根据用户查询来定位相关文档，这些系统通常由三个独立的模块组成:索引模块、检索模块和重新排序模块。三个独立的模块难以在整体上进行优化从而可能会导致次优的结果。

基于模型的检索方法尝试解决这样的挑战，这些方法脱离了传统的“索引-检索-排名”模式，而是使用统一的模型直接生成与查询相关的文档标识符(即DocIDs)，在这些基于模型的生成检索方法中，文档语料库的知识存储在模型参数中，从而消除了对索引额外存储空间的需求。

目前主要有两类方法尝试来生成文档标识符：

微调方法
提示方法
微调方法
参考案例：DSI

DSI是一种在检索数据集上对预训练的T5模型进行微调的典型方法。
该方法包括对查询进行编码并直接解码文档标识符以执行检索。
发现构造语义结构化标识符可以产生最佳结果。
DSI根据文档的语义嵌入应用分层聚类对文档进行分组，并根据其分层组为每个文档分配语义DocID。
为了确保输出的DocID是有效的，并且确实表示语料库中的实际文档，DSI使用所有DocID构造一个trie（字典树），并在解码过程中使用约束束搜索。
尺度定律也适用于生成检索器
提示方法
除了对LLM进行检索微调外，还发现LLM(例如gpt系列模型)可以通过一些上下文演示直接为用户查询生成相关的web url，因此，LLM自然可以作为生成式检索器，直接生成文档标识符以检索输入查询的相关文档。

案例：LLM-URL。

它利用GPT-3 text-davinci-003模型生成候选url，然后设计正则表达式来从这些候选对象中提取有效的url，以定位检索到的文档。

检索领域的现存限制
快速响应的需求与大模型参数过大的不匹配
LLM生成的文本与实际的用户查询需求存在潜在的不匹配
应用在特定的下游任务之前，需要对特定任务数据集进行微调。
Reranker
Reranker作为IR中的第二遍文档过滤器，旨在根据查询文档相关性对检索器检索到的文档列表进行重新排序。

现有的reranker方法一般可以分为以下三个维度：

微调LLM进行重排序
提示LLM进行重排序
利用LLM进行训练数据增强
微调方法
现有的LLM微调策略可以分为两种主要方法:

微调LLM作为生成模型
微调LLM作为排名模型。
生成方法
方法一
参考文献：Document ranking with a pretrained sequence-to-sequence model

对T5模型进行微调，以便为相关或不相关的查询文档对生成分类令牌。
在推理时，将softmax函数应用于“真”和“假”令牌的逻辑，并将相关分数计算为“真”令牌的概率。
方法二：一种基于T5模型的多视图学习方法
参考文献： Text-to-text multi-view learning for passage reranking

此方法同时考虑两个任务:为给定的查询-文档对生成分类令牌，并根据所提供的文档生成相应的查询。
DuoT5将三元组( q , d i , d j ) (q, d_i, d_j)(q,d 
i

 ,d 
j

 )作为T5模型的输入，并进行了微调，如果文档di比文档dj与查询qi更相关，则生成令牌“true”，否则生成令牌“false”。
在推理过程中，对于每个文档d i d_id 
i

 ，它枚举所有其他文档d j d_jd 
j

 ，并使用全局聚合函数生成文档d i d_id 
i

 的相关性评分s i s_is 
i

 (例如s i = ∑ j p i , j s_i =\sum_j p_{i,j}s 
i

 =∑ 
j

 p 
i,j

 ，其中pi,j表示以( q , d i , d j ) (q, d_i, d_j)(q,d 
i

 ,d 
j

 )作为模型输入时生成“true”的概率)。
排名方法
背景：尽管将微调LLM作为生成模型的方法优于一些强大的排名基线，但对于重新排名任务来说，它们并不是最优的。

原因：

期望重新排序模型将为每个查询文档对生成数字相关性评分，而不是文本标记。
与生成损失相比，使用排序损失(例如RankNet)来优化重排序模型更为合理。
虽然一些预训练模型(如BERT)已经被用于文档重新排序，但使用基于seq2seq的LLM(如T5-3B)进行重新排序任务尚未得到彻底的研究，RankT5直接计算查询-文档对的相关性评分，并通过“成对”或“列表”排序损失来优化排序性能，目前认为使用更大的模型可能会带来更大的提升潜力。

提示方法
提示方法主要分为以下三个模块：

点法（Pointwise methods）
列表法（Listwise methods）
成对法（Pairwise methods）
数据增强方法
目前，该领域主要有四类解决方案：

ExaRanker使用GPT-3.5为检索数据集生成解释，随后训练一个seq2seq排序模型，为给定的查询文档对生成相关标签以及相应的解释。
InPars-Light被认为是通过提示LLM来合成文档查询的一种经济有效的方法。
ChatGPT-RetrievalQA是通过响应用户查询生成基于LLM的合成文档来构建新的数据集。
将ChatGPT的文档排序能力提炼成一个专门的模型，首先指示ChatGPT直接生成文档的排名列表，然后，使用生成的文档列表作为目标，使用各种排名损失(例如RankNet)来训练学生模型(即DeBERTa-v3-base)。参考文献：https://arxiv.org/abs/2304.09542
限制
成本问题，如何最小化对LLM api的调用。
虽然现有的研究主要集中于将LLM应用于开放域数据集(如MSMARCO)或基于相关性的文本排序任务，但其对域内数据集和非标准排序数据集的适应性仍然是一个需要更全面探索的领域。
Reader
作用：基于文档语料库生成答案。

两种简单的实现思路
启发式地为LLM提供与用户查询或先前生成的文本相关的文档，以支持后续生成。（被动方法）
训练LLM主动与搜索引擎进行交互。（主动方法）
被动阅读器
为了为用户生成答案，一种简单的策略是根据查询或先前从IR系统生成的文本提供检索到的文档，作为LLM创建段落的输入，通过这种方式，这些方法以不同的方式使用LLM和红外系统，LLM作为红外系统文档的被动接受者。

单次检索的被动阅读器
定义：在开始时直接根据查询本身检索对应的top message。

REALM直接将文档内容与原始查询联系起来，基于掩码语言建模来预测最终答案。

RAG遵循这一策略，但采用生成语言建模范式。

上述两种方法仅使用具有有限参数的语言模型，如BERT和BART。

REPLUG和Atlas，通过利用GPT和T5等LLM来生成响应，对它们进行了改进。为了产生更好的答案生成性能，这些模型通常会对QA任务上的LLM进行微调，然而，由于计算资源有限，许多工作选择提示生成LLM，因为这样可以使用更大的语言模型。

Periodic-Retrieval Reader（定期检索阅读器？）
动机：在生成长结论性答案的同时，仅使用原始用户意图检索的参考文献作为一次检索阅读器可能是不够的，在生成过程中，语言模型可能需要额外的引用来支持下一个生成，其中可能需要多个检索过程。

两种主流的方法和一类探索
RETRO和RALM共有的
RETRO 和 RALM 基于原始查询和并发生成的文本定期收集文档(每n个生成的令牌触发检索)，这种对额外引用的需求强调了多次检索迭代的必要性，以确保后续答案生成的鲁棒性。

RETRO特有的
RETRO引入了一种新颖的方法，在Transformer注意力计算中结合了生成文本和引用之间的交叉注意，而不是直接将引用嵌入到LLM的输入文本中，由于它在Transformer的结构中涉及到额外的交叉关注模块，RETRO从头开始训练这个模型。

缺陷
这两种方法主要依赖于连续的n个令牌来分离生成和检索文档，这可能不是语义连续的，并且可能导致收集到的引用嘈杂和无用。

一类新的尝试
IRCoT开始探索为每个生成的句子检索文档，这是一个更完整的语义结构。

Aperiodic-Retrieval Reader（非周期检索阅读器？）
定期检索阅读器存在的问题：以强制频率检索文档可能与检索时间不匹配，并且代价高昂。

非周期检索的代表性工作：FLARE

FLARE
由于概率可以作为LLM在文本生成过程中的信心指标，生成的术语的低概率可能表明LLM需要额外的知识，当一个术语的概率低于预定义的阈值时，FLARE使用IR系统根据正在生成的句子检索引用，同时删除这些低概率术语，通过采用这种策略，仅根据生成术语的概率来提示LLM生成答案，避免了微调的需要，同时仍然保持有效性。



关于被动阅读器的总结
主动阅读器（ Active Reader）
被动阅读器存在的问题：将IR系统和生成语言模型分开，这意味着LLM只能顺从地利用IR系统提供的参考，而不能以类似于人类交互的方式与IR系统进行交互，例如发出查询以寻求信息。

代表性成果
少量提示触发查询
代表作品：SelfAsk和DSP

他们尝试为LLM使用少量提示，在LLM认为有必要时触发他们搜索查询，这两个方法涉及到使用信息检索系统来构建整个LLM的单个推理链条。

改进作品：MRC

MRC进一步改进了这些方法，促使LLM探索多个推理链，随后使用LLM将所有生成的答案组合在一起。

自动使用搜索引擎
WebGPT采用了另一种方法，即训练GPT-3模型自动使用搜索引擎，而不是提示LLM，它通过应用强化学习框架实现的，其中为GPT-3模型构建了模拟环境，WebGPT模型使用特殊的令牌来执行诸如查询、滚动排名和引用搜索引擎上的引用等操作。

阅读器目前存在的缺陷
有效的查询重新表述
最佳检索频率
正确的文档理解
准确的段落提取和有效的内容摘要
未来探讨
查询重写器
Rewriting query according to ranking performance.
缺乏对重写查询的最终检索质量的意识
Improving query rewriting in conversational search.
到目前为止，主要的工作是改进ad-hoc搜索中的查询重写
相比之下，会话搜索呈现出更发达的景观，为LLM提供了更广泛的范围，以促进查询理解
通过整合历史交互信息，LLM可以根据用户偏好调整系统响应，从而提供更有效的会话体验。
此外，LLM还可以用于模拟会话搜索场景中的用户行为，提供更多的训练数据，这是当前研究中迫切需要的。
Achieving personalized query rewriting
LLM通过分析用户特定数据的能力为个性化搜索提供了宝贵的贡献，在查询重写方面，凭借LLM出色的语言理解能力，可以利用它们根据用户的搜索历史(例如，发出的查询、点击行为和停留时间)构建用户配置文件，这允许实现个性化查询重写，以增强信息检索，并最终有利于个性化搜索或个性化推荐。
检索器
减少延时或者说将LLM模型的能力向小模型转移
模拟真实的查询以进行数据扩展
生成的查询数据与用户的行为进行对齐，引入强化学习，使得LLM尽可能的发出真实的查询数据
Incremental indexing for generative retrieval
LLM参数的静态特性，再加上昂贵的微调成本，给添加新文档时更新生成检索器中的文档索引带来了挑战。
因此，探索构建增量索引的方法是至关重要的，它允许在基于LLM的生成检索器中进行有效的更新。
现有的模型基本只是支持文本的查询，对于多模态的检索是一个由开创性意义的工作
Reranker
Enhancing the online availability of LLMs
在线部署LLMs很困难，许多模型会考虑调用LLM api，但是这个会带来相当大的成本，设计有效方法来增强大模型成为了一个值得探索的问题（如大模型的蒸馏等）
个性化的排名方案
通过分析用户的搜索历史，LLM可以构建准确的用户档案，并据此对搜索结果进行重新排序，从而提供个性化的、用户满意度更高的结果。
Adapting to diverse ranking tasks.
Reader
提高参考文献的质量.
当前的检索文献种存在与查询无关的噪声，有必要从检索的文档中提取相关的片段，以增强文章的性能。
提高回答的可靠性
关于信息检索系统的评价
assessing ranking performance
evaluating generation performance
总结
我们根据现有方法的功能将它们分成不同的类别:query rewriting, retrieval, reranking, reader四个模块。

在查询重写领域，LLM已经证明了它们在理解歧义或多面查询方面的有效性，提高了意图识别的准确性。
在retrieval中，LLM通过在考虑上下文的情况下支持查询和文档之间更细微的匹配，提高了检索准确性。
在重新排序领域，LLM增强的模型在重新排序结果时考虑更细粒度的语言细微差别。
在IR系统中纳入阅读器模块是朝着生成全面响应而不仅仅是文件列表迈出的重要一步。
从查询重写到检索、重新排序和阅读器模块，LLM通过高级语言理解、语义表示和上下文敏感处理丰富了IR过程的各个方面，随着这一领域的不断发展，IRLLM的旅程预示着一个更加个性化、精确和以用户为中心的搜索体验的未来。
————————————————
版权声明：本文为CSDN博主「玩＊＊玩的」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/lzz10081203/article/details/134493624