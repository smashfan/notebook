Evaluating performance of AI operators using roofline model

introdunction

人工智能（AI）算法已经在广泛的应用领域显示出性能优势，如计算机视觉[1]、自然语言处理[2]和语音识别[3]。卷积神经网络（CNN）[1,4-7]、递归神经网络（RNN）、长短时记忆（LSTM）[8]、生成对抗网络（GAN）[9]和几种机器学习算法[10-12]是代表性算法。



基于硬件的加速器被广泛用于加速人工智能算法，其中大部分是以CPU[13]、GPU[14,15]、FPGA[ 16-18]和ASIC[19-22]的形式存在。目前，有两种人工智能加速芯片：训练芯片和推理芯片。训练芯片执行人工智能算法中涉及的所有操作，而推理芯片只执行与推理有关的操作。由于GPU和ASIC的性能优势和功率效率，市场上的主流AI加速器以基于GPU和ASIC的加速器为主。NVIDIA Tesla V100[14]和A100[15]是提供高训练能力的代表性GPU。它们将强大的张量核心与高速、高带宽的内存整合在一起。谷歌的张量处理单元（TPU）[23]是一个著名的基于ASIC的深度学习芯片，使用高效的架构--收缩阵列[24]--来实现高计算能力。Cambricon-X[25]是一个训练加速器，可以大大加速稀疏神经网络的训练。



人工智能应用的一个典型特征是追求更复杂的算法、更深的神经网络和更大的数据集规模。这一趋势对人工智能加速器的设计提出了挑战。为了跟上人工智能应用不断增长的需求，学术界/在线发表：2021年9月20日《应用智能》（2022）52：7054-706已经提出了很多研究。其中，在加速器上评估人工智能算法的性能是一个热门话题。例如，Jouppi等人在TPU、K80和Haswell上运行两个CNN、两个MLP和两个LSTM[23]。他等人在Cambricon上评估并验证了ResNet的性能[26]。然而，这样的工作通常需要杂七杂八的实验设置和配置，并可能涉及重复的测试。
