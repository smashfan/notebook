### 背景

- 以往的研究大多数基于固定的骨架图，仅仅捕获**关节之间的局部物理关系**，这样可能会丢失隐性的关节点相关性。

  

- 最早动作识别尝试通常将每个帧中的所有身体关节位置编码为用于模式学习的特征向量

- 

为了捕捉更丰富的依赖关系，论文介绍了一种编解码结构，称为A-link推理模块。直接从动作中捕捉特定的潜在依赖关系，即动作链接。作者还拓展了现有的骨架图来表示更高阶的依赖关系，即结构链接。将这两种类型的链接组合成一个广义骨骼图，作者进一步提出了动作结构卷积网络(Actional-Structural, AS-GCN),网络将动作结构图卷积和时间卷积作为一个基本的构建块(a basic building block), 学习动作识别的时空特征。在识别的同时增加一个未来姿势预测头(prediction head),通过自我监督帮助捕捉更详细的动作模式。



时空图卷积网络(ST-GCN)更好的发展了GCN特性，同时学习了空间和时间特征。但是，ST-GCN虽然提取了直接通过骨骼连接的关节特征，但是在结构上较远的关节(可能包括关键动作的模式)却被忽略了。