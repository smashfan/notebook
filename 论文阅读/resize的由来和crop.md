





几何变换的一个常规操作是要把图片的大小resize到相同尺寸，便于统一处理。同时也是要考虑到机器的配置，图片越大在进行运算处理时要求的配置就越高，常规情况下我们会把图片resize到500以下进行处理。很多预训练模型的图片size大小为299×299×3（Xception、InceptionV3…）、224×224×3（VGG16、ResNet50…）。




- 将输入图像resize到一个固定的尺寸这件事贯穿了DL在目标检测领域的始终：

  ​	

1. 在**r-cnn**刚提出的阶段，由于网络结构的限制，进入 **全连接层的输入维度必须是固定的**，那么一个最简单的解决方案就是把输入图像归一化到固定的尺寸，得到输出后反变换回去。这种resize的做法实际上是为了适应卷积神经网络的一种让步。

   2.随着人们对检测精度的追求，大家开始关注resize所带来的损失。不论是sppnet还是roi pooling，他们都致力于将任意维度的特征图转化成固定的维度以适应全连接的输入，这一思路从理论上解除了整个检测网络对输入的尺寸依赖，这也是fast r-cnn的一个改进点，从此输入图像可以是任意宽高。


3. 然而各路算法在使用gpu做加速的过程中遇到了新的问题，每一个batch的计算必须拥有相同的size，这是算法本身之外的一个限制，但这个限制却成为了并行加速计算的拦路虎。yolo的作者针对这个情况，在不改变原始图像宽高比的前提下进行了resize操作，具体的做法是对不符合原图比例的区域进行padding。
   ————————————————

**首先我们要知道crop的目标是用于做数据增强。**

当我们描述一个数据集的大小，通常是以数量级来衡量，而数据增强也比例外，我们的目标是将数据集增长为原来的几个数量级以上，即扩充10，100，1000倍等。

输入为N*N的大图，crop输出为M*M的小图，可以实现(N-M)*(N-M)的数据集扩充倍数。对应扩充10，100，1000，10000倍，N-M约为3，10，32，100，因为最终结果图是224*224，所以原图就应该是227*227，234*234，256*256，324*324，很明显从256*256这个尺度进行裁剪，能够保证主体不至于太小或者太大，如前面图中红色框。



那不使用256*256使用其他的尺寸如何呢？在比赛刷榜中，经常使用多尺度模型测试，即使用不同尺度的输入图进行裁剪然后进行结果融合，笔者曾经在Place365数据集上训练过ResNet和DPN模型，下面是不同尺度的测试结果。



不同尺度的结果会有差异，但是通常都不大，因此除非模型特殊，不必太纠结与这个问题，毕竟256等于2^8，2的指数次幂，多么经典又快意。

### 参考文献

https://blog.csdn.net/weixin_42535423/article/details/103720514

https://blog.csdn.net/hacker_long/article/details/100138536