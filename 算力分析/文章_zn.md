### augur文章

​	relate work：尽管CNN已经被应用于不同计算平台上的各种计算机视觉应用，但只有少数作品考虑在移动设备上运行CNN，我们设想这将是未来部署深度学习应用的一个重要领域。在这些工作中，许多人专注于加速CNN的计算，例如，通过压缩参数[15]，[24]，[34]，通过云卸载[16]，[23]，以及通过将计算分布到机载异构处理器（GPU，DSP）[12]，[19]，[26]。有些人考虑减少内存的使用[13]、[20]、[30]和能源[11]，同时保持高推理精度。在移动设备上运行CNN的资源瓶颈在[27]中得到了初步的研究。在[9]中对不同的CNN进行了基准测试，但是它并没有考虑如何对CNN的计算需求进行建模。当CNN从几层发展到一千层的时候，移动设备的计算能力也在不断提高。因此，不同的移动设备在不同的CNN上有不同的表现，因此可能需要也可能不需要定制的优化和卸载，这取决于一个CNN是否可以在一个给定的移动平台上运行以及效率如何。这个问题激发了我们的工作。





摘要--卷积神经网络（ConvNets/CNNs）已经彻底改变了计算机视觉的研究，因为它们有能力捕捉复杂的模式，从而获得高推理精度。然而，这些神经网络日益复杂的性质意味着它们特别适合于具有强大GPU的服务器计算机。我们设想，深度学习的应用最终将被广泛部署在移动设备上，例如智能手机、自动驾驶汽车和无人机。因此，在本文中，我们旨在了解移动设备上的CNN在计算时间、内存和功率方面的资源需求。首先，通过在不同的移动CPU和GPU上部署几个流行的CNN，**我们测量和分析了CNN的性能和资源使用情况**，**并以层为单位。我们的发现指出了在移动设备上优化CNN管道的潜在方法**。其次，我**们对CNN的核心计算的资源需求进行了建模**。最后，在测量和建模的基础上，我们建立并评估了我们的建模工具Augur，该工具以CNN配置（描述符）为输入，估计CNN的计算时间、内存和功率需求，以深入了解CNN是否可以在给定的移动平台上运行以及效率如何。



我们的动机是，我们的系统可以提供指导方针，以决定何时需要进行性能优化、卸载等，以便在移动设备上成功运行分析任务。例如，使用我们模型的输出，人们可以决定在移动设备上运行所有的卷积层，而将完全连接的层卸载到云端，以减少移动设备上的内存需求。尽管准确地对CNN的性能和资源使用进行建模是非常困难的，但我们在实现这一目标方面取得了进展



### 我的：

​	我们的动机是，我们的预测模型可以对神经网络模型在移动设备上运行性能得到精准的判断，便于做模型设备适配，通过使用我们的结果，，在大量模型调参和设备选择中提供参数   ，也包括模型调度，任务调度做前提，最终达到加速模型的目的，

我们在模型轻量化IOT设备前

### nn-meter文章

为了预测CNN模型的延迟，nn-Meter将模型图分割成内核，并将预测的内核延迟加起来作为模型延迟。

​	：不知道运行时的实现。目前的CNN设计使用高层次的API，这与运行时的实现无关。此外，大多数运行时是闭源的。因此，许多CNN延迟预测器只依赖于CNN模型的特征，而没有考虑运行时的实现。一些工作[15, 22, 23, 30]仅仅使用模型的FLOPs和MAC作为延迟的代理，或者使用这些作为回归器[28]的特征输入来预测延迟。然而，这些方法是不准确的，因为它们忽略了各种操作者的运行时间行为差异。与我们的论文类似，NeuralPower[5]和PALEO[27]预测运算符或层的延迟，并将它们汇总为模型延迟。由于忽略了运行时的模型图优化，它们比nn-Meter差。

​	

​	BRP-NAS[13]通过将运算器类型和图形作为特征编码到GCN预测模型中，可以同时学习运算器延迟和图形优化。然而，正如我们所显示的，它对具有不同数量运算符的新CNN模型的概括能力很低，而且它能学习的连接距离也很有限。

​	对运算符实现的预测。一些算子延迟预测器使用机器学习方法从低级别的实现中学习延迟。他们或者使用代码特征和简单的回归模型来预测操作员延迟[1, 16]，或者使用昂贵的DNN代码嵌入[19, 25]方法来避免特征工程。TVM[10]使用两种方法来加速其代码搜索过程。它的基于嵌入的延迟预测器递归地使用TreeGRU模型将低级AST嵌入到一个向量中，然后使用线性层将其映射到预测的延迟。另一个预测器使用代码特征，如内存访问、数据重用、矢量化和解卷，作为XGBoost模型的输入来预测延迟。然而，由于大多数边缘DNN运行时是闭源的，使用这些基于代码的方法是不可行的。



还有语言编译器（如LLVM-MCA[3]和IACA[18]）和周期精确的硬件模拟（如gem5[4]和GPGPUSim[20]）普遍使用的分析性延迟预测方法。这些方法需要对处理器的确切机制有所了解，这对于黑盒边缘人工智能硬件来说也是不可行的。

### relate work

​	硬件复杂性：

预测复杂性：

​	随着越来越多的智能端设备接入到我们的生活当中，各种轻量化CNN[1]网络被应用在嵌入式设备和移动手机上，

 早期的轻量化网络【1，2，3】根据作为模型复杂程度以及延迟的估计，通过不断降低flop和mac来优化网络结构，但是依然不能预测出神经网络模型在设备上真实的运行时延。



###   augur :

​	augur框架对神经网络在移动平台的核心计算进行建模，对时延和功耗进行了线性拟合,



#### 	nn-Meter

将模型图分割成内核，并将预测的内核延迟加起来作为模型延迟。但是没有联系到硬件设备的实时信息，比如cache信息，带宽，cpu利用率等等，并且预测延迟没有考虑，不合适实时

 

​	





参考目录

[1] mobilenet

[2]SqueezeNet

[3]ShuffleNet

[4]augur