### augur文章

​	relate work：尽管CNN已经被应用于不同计算平台上的各种计算机视觉应用，但只有少数作品考虑在移动设备上运行CNN，我们设想这将是未来部署深度学习应用的一个重要领域。在这些工作中，许多人专注于加速CNN的计算，例如，通过压缩参数[15]，[24]，[34]，通过云卸载[16]，[23]，以及通过将计算分布到机载异构处理器（GPU，DSP）[12]，[19]，[26]。有些人考虑减少内存的使用[13]、[20]、[30]和能源[11]，同时保持高推理精度。在移动设备上运行CNN的资源瓶颈在[27]中得到了初步的研究。在[9]中对不同的CNN进行了基准测试，但是它并没有考虑如何对CNN的计算需求进行建模。当CNN从几层发展到一千层的时候，移动设备的计算能力也在不断提高。因此，不同的移动设备在不同的CNN上有不同的表现，因此可能需要也可能不需要定制的优化和卸载，这取决于一个CNN是否可以在一个给定的移动平台上运行以及效率如何。这个问题激发了我们的工作。





摘要--卷积神经网络（ConvNets/CNNs）已经彻底改变了计算机视觉的研究，因为它们有能力捕捉复杂的模式，从而获得高推理精度。然而，这些神经网络日益复杂的性质意味着它们特别适合于具有强大GPU的服务器计算机。我们设想，深度学习的应用最终将被广泛部署在移动设备上，例如智能手机、自动驾驶汽车和无人机。因此，在本文中，我们旨在了解移动设备上的CNN在计算时间、内存和功率方面的资源需求。首先，通过在不同的移动CPU和GPU上部署几个流行的CNN，**我们测量和分析了CNN的性能和资源使用情况**，**并以层为单位。我们的发现指出了在移动设备上优化CNN管道的潜在方法**。其次，我**们对CNN的核心计算的资源需求进行了建模**。最后，在测量和建模的基础上，我们建立并评估了我们的建模工具Augur，该工具以CNN配置（描述符）为输入，估计CNN的计算时间、内存和功率需求，以深入了解CNN是否可以在给定的移动平台上运行以及效率如何。



我们的动机是，我们的系统可以提供指导方针，以决定何时需要进行性能优化、卸载等，以便在移动设备上成功运行分析任务。例如，使用我们模型的输出，人们可以决定在移动设备上运行所有的卷积层，而将完全连接的层卸载到云端，以减少移动设备上的内存需求。尽管准确地对CNN的性能和资源使用进行建模是非常困难的，但我们在实现这一目标方面取得了进展





我们需要解析一个CNN的描述符。**输入的维度（如图像和特征图**）和**网络参数（如卷积核）决定了CONV或FC层的两个矩阵大小（**要乘以的）。由于特征图的尺寸可以由其他一些层改变，例如POOL层，我们需要逐层追踪特征图的尺寸。然而，这可以通过解析每一层的参数设置，如零填充（P）、跨度（S）、输出特征图的数量（N）来轻松完成。例如，在CONV层的情况下，让I表示输入特征图的空间尺寸，O表示输出特征图的空间尺寸，K表示卷积核的三维体积



### 我的：

​	我们的动机是，我们的预测模型可以对神经网络模型在移动设备上运行性能得到精准的判断，便于做模型设备适配，通过使用我们的结果，，在大量模型调参和设备选择中提供参数   ，也包括模型调度，任务调度做前提，最终达到加速模型的目的，

我们在模型轻量化IOT设备前

### nn-meter文章

为了预测CNN模型的延迟，nn-Meter将模型图分割成内核，并将预测的内核延迟加起来作为模型延迟。

​	：不知道运行时的实现。目前的CNN设计使用高层次的API，这与运行时的实现无关。此外，大多数运行时是闭源的。因此，许多CNN延迟预测器只依赖于CNN模型的特征，而没有考虑运行时的实现。一些工作[15, 22, 23, 30]仅仅使用模型的FLOPs和MAC作为延迟的代理，或者使用这些作为回归器[28]的特征输入来预测延迟。然而，这些方法是不准确的，因为它们忽略了各种操作者的运行时间行为差异。与我们的论文类似，NeuralPower[5]和PALEO[27]预测运算符或层的延迟，并将它们汇总为模型延迟。由于忽略了运行时的模型图优化，它们比nn-Meter差。

​	

​	BRP-NAS[13]通过将运算器类型和图形作为特征编码到GCN预测模型中，可以同时学习运算器延迟和图形优化。然而，正如我们所显示的，它对具有不同数量运算符的新CNN模型的概括能力很低，而且它能学习的连接距离也很有限。

​	对运算符实现的预测。一些算子延迟预测器使用机器学习方法从低级别的实现中学习延迟。他们或者使用代码特征和简单的回归模型来预测操作员延迟[1, 16]，或者使用昂贵的DNN代码嵌入[19, 25]方法来避免特征工程。TVM[10]使用两种方法来加速其代码搜索过程。它的基于嵌入的延迟预测器递归地使用TreeGRU模型将低级AST嵌入到一个向量中，然后使用线性层将其映射到预测的延迟。另一个预测器使用代码特征，如内存访问、数据重用、矢量化和解卷，作为XGBoost模型的输入来预测延迟。然而，由于大多数边缘DNN运行时是闭源的，使用这些基于代码的方法是不可行的。



还有语言编译器（如LLVM-MCA[3]和IACA[18]）和周期精确的硬件模拟（如gem5[4]和GPGPUSim[20]）普遍使用的分析性延迟预测方法。这些方法需要对处理器的确切机制有所了解，这对于黑盒边缘人工智能硬件来说也是不可行的。



### High-Throughput CNN文章



CNN的发展正朝着更复杂的网络结构和适度的资源需求发展。从2012年AlexNet[16]的250MB开始，模型的大小已经减少到2016年SqueezeNet[13]的0.5MB以下而不失准确性。这样的进步使得CNN可以在移动平台上部署，即使其计算和内存资源有限。为了有效地在嵌入式平台上部署CNN，研究人员从不同的角度出发。网络结构被修改以适应资源受限的移动平台，如量化[32]，加速计算并减少内存使用，以及网络修剪[33]，在减少资源需求的情况下损害了准确性。此外，在NN应用中利用稀疏性[25]来减少计算量并提高边缘设备的执行性能。



加速器能够在边缘设备上高度节能地执行CNN。一些工作依靠嵌入式GPU的计算能力，使CNN在CPU和其他处理器上协同执行。DeepX[17]框架通过在多个处理器（包括GPU和低功耗处理器（LPU））上协同执行来实现边缘的NN。它首先使用运行时层压缩来控制NN工作负载的资源需求。然后，它将工作负载分解成单元块，分配给多个处理器。DeepX主要从其全连接层中获得AlexNet的性能和能源方面的巨大优势。现在，在最先进的CNN中，全连接层的使用已经很少。DeepSense[11]和DeepMon[10]提出了一个基于OpenCL的移动GPU框架。DeepSense采用了GPU内存管理技术，加速了计算量大的执行，包括在GPU上执行卷积和全连接层。DeepMon扩展了DeepSense，包括进一步的缓存优化并改进了卷积层的执行。现在正在设计专门用于神经网络处理的ASIC，如谷歌的Tensor处理单元（TPU）和华为的神经处理单元（NPU）。研究人员还结合具体应用的特点共同设计算法和架构[34], [35] 。

研究人员已经描述了CNN的资源需求[18], [22]，为设计有资源限制的CNN提供了见解。高效的库[1]、[3]、[6]、[19]被创建，以促进边缘设备上深度学习的实施。像CGOOD[14]这样的框架，通过自动生成C和GPU（CUDA或OpenCL）代码，在具有硬件规格和优化要求的各自平台上运行，来促进边缘设备上CNN的部署。



另一方面，旧的技术节点或对成本敏感的平台，缺乏有能力的GPU和加速器，仍然需要通过其CPU执行CNN。Graphi[30]提出了一个框架，通过多核上的NN内的层级并行性来加速深度学习模型。它利用了网络结构中固有的层级并行性，并将独立的层安排为并发执行。Graphi有利于诸如LSTM和GoogLeNet等具有高度层级并行性的网络。相比之下，Pipeit关注的是计算内核级的并行性。它适用于一般的网络结构，目标是在异构多核上进行CNN加速。

---

### Abstract

### introdunction

### background and motivation

背景：

【物联网的发展】

​	据 IoT Analytics 公布的调研数据显示，2020年最终企业用户对物联网解决方案的总支出达到1289亿美元，预计到2021年底将达到1598亿美元（约合1万亿元人民币）。随着工业3.0时代的到来，端边设备能力的改善（如算力、内存容量、能耗），以及AI技术在目标检测、目标跟踪、图像分割、场景分类、人脸识别、姿态估计、动作与行为识别等细分方向的突破，进一步加快了AIOT商业应用的扩展和行业成熟度的提升。在端边云和联邦学习的场景下，端侧部署高效轻量网络不仅可以减少云端网络传输通信量，减少边云的计算需求还能够加强隐私的保护效果。

【神经网络的发展】

​	自从alexnet网络在图像识别中卓有成效以来，陆续发布各种各样的cnn网络，如vgg，googlenet，resnet，densenet，squeezenet，mobilenet，特别在网络的参数量和精确度上取得了很好的平衡。

【推理引擎框架】

​	近年来，随着人神经网络的迅速发展，神经网络推理引擎框架成为了越来越重要的技术领域。其中，TVM、MNN、NCNN、TFLite和MindSpore Lite等是目前较为热门的神经网络推理引擎框架。这些框架为各种应用场景提供了高效、快速、可靠的神经网络推理功能，为实现神经网络广泛应用于端边提供了强有力的支持。

【arm大小核架构】	

​	目前市场上移动手机的处理器为了在功率和性能取得平衡，多采用大小核架构的八核异构cpu，这种架构可以适应高处理强度的时期，比如在移动游戏和网页浏览中看到的那些时期，而低处理强度的任务(比如短信、电子邮件和音频)的时期通常更长，在复杂的应用程序期间则处于静止状态。例如图[1]直观的展示了晓龙888u的八核异构多核，他将8个核心分为大中小三个集群，大核集群采用一枚2.84GHz的 cortex X1核心拥有1mb的L2 cache，中核集群采用的是 3枚2.4GHz Cortex A78拥有 512KB 的二级缓存，小核集群采用的是4枚1.8GHz cortex A55拥有128KB L2 缓存。CPU 还配备了 4MB 的 L3 [缓存](https://baike.baidu.com/item/缓存/100710?fromModule=lemma_inlink)和 3MB 的系统缓存。

挑战：

​	硬件环境和系统环境的复杂性：由于系统环境对性能的影响，调度器会因为模型在cpu上的占用率不足被调度到中核或者小核、内存缺页等从而带来性能损失。尽管可以通过绑定特性的核和使用内存池来有限的解决问题。但当系统中运行了其他进程时也会对当前进程造成不可避免的影响。

​	性能建模的困难：由于影响性能的因素很多，不能仅仅认为性能和flops和mac成线性关系，实际上和卷积的各种参数，设计模式如分离卷积，残差卷积都有着各种非线性关系，导致在评估性能时很难给出一些普适性的结论。所以很难快速而准确的对其性能进行建模。

​	我们的动机是：我们的预测模型可以对神经网络模型在任何已知移动设备上运行性能得到精准的预估，以便于做模型设备适配，模型优化，任务优化，端侧加速，

​		硬件复杂性：

预测复杂性络【1，2，3】根据作为模型复杂程度以及延迟的估计，通过不断降低flop和mac来优化网络结构，但是依然不能预测出神经网络模型在设备上真实的运行时延。



### relate work

> 两个方面 ，一个是智能物联网的发展，一个是cnn网络的发展

**开场白：**

​	随着智能物联网技术*的发展，数量、种类繁多的物联网设备正在渗透我们的我们，如移动手机，人脸识别摄像头，智能穿戴设备等，其中也伴随着CNN网络追求更深层，更复杂追求更高精准度如resnet，转向追求更加亲和算力资源受限的端边设备如mobilenet。为了让在常见CNN网络在移动平台成功部署，科学家一般采用蒸馏，减枝，量化压缩网络来平衡网络的参数量和精确度。进一步的通过预估cnn网络在设备上的性能来进优化网络，但是由于硬件的异构和种类繁多，如在cpu，gpu，npu结合的处理器上，科学家很难评估cnn网络在不同硬件设备上性能。这个问题激发我们的工作。

####   augur :

​	augur框架为了深入了解CNN是否可以在给定的移动平台上运行以及效率如何，测量和分析了CNN的性能和资源使用情况，该工具对CNN的核心计算的资源需求进行了建模，以CNN配置（描述符）为输入，估计CNN的计算时间、内存和功率需求。然而它仅只对神经网络的fc和conv层中的矩阵乘法建立了简单的线性模型，没有考虑更多的因素，例如，非矩阵操作（缩放、池化等）、内存操作（memcpy、im2col、col2im等）、卷积类型（纵深或通道洗牌）和CNN架构（堆叠或分支），并且仅考虑tk1和tx1特定的硬件，即没有考虑硬件动态变化，也没有现有推理引擎技术如ncnn ，mnn, tflite等 对其的优化推理加速的影响。

#### **Performances**

​	通过机器学习来提高预测性能的方法在这片文章提出了，它给出一种在边缘GPU上的预测CNN模型性能的方法。首先建立经典和先进CNN的模型数据集，提取出重要CNN特征（卷积层和全连接层的数量、输入图像的大小和神经元的数量等），以及收集边缘设备的测试数据时间，内存使用，功耗，最后通过每次从零添加新的特征到机器学习模型进行训练，直到训练得到的模型误差不下降为止，从而得到一个精度较高的结果。尽管考虑了很多模型的特征因素，但对不同硬件信息的影响依旧没有考虑到。

#### 	nn-Meter

​    nn-meter将模型图分割成内核，并将预测的内核延迟加起来作为模型延迟。它首次在预测性能的时候结合了推理引擎的影响，所以得到了目前为止比较好的精度，但是没有联系到硬件设备的实时信息，比如cache信息，带宽，cpu利用率等等，并且整个框架预测需要分割内核，较为复杂，预测延迟没有控制，不合适实时端侧优化加速

#### high

​	对卷积层的时延预测，并以此来优化加速整个ai任务的工作，已经在这篇文章有所体现，在arm大小核架构的处理器上通过预测卷积层的时延来将卷积层跨集群分割，同时将其各自内核的并行化限制在指定的集群中。有效地利用了异构多核的所有片上内存和处理资源，从而提高了吞吐量。



​	硬件复杂性：

预测复杂性络【1，2，3】根据作为模型复杂程度以及延迟的估计，通过不断降低flop和mac来优化网络结构，但是依然不能预测出神经网络模型在设备上真实的运行时延。



### The Design

In this section, we describe the overall of system and the benchmark dataset collection.

概述。图3说明了系统结构。它显示了两个核心组件，以实现CNN模型的精确延迟预测。它包括特征提取和性能建模部分。前者会对目标模型和目标设备提出重要的特征，后者会对提取出来的特征进行机器学习来预测它的时延。

基准数据制作：为了评估我们的系统在任意cnn模型上都具有适用性，我们自己制作了一个基准数据集，其中包括经典模型以及它的变种模型文件本身，准备用它在不同硬件上做测试。nn-meter同样制作了数据集，但它的IR仅适用于nn-meter代码，所以我们这里制作更为通用的模型数据集。我们的数据集共有10种经典网络，其中包含了conv、pooling、relu、和fc等各种算子。

​	在这项工作中，我们考虑各种不同的网络结构，生成了一个比较大的数据集，确保我们的系统可以在任何模型结构下都保持有效性，首先，我们在imagenet2012上收集了10个最经典以及最先进的CNN模型。他们包含了不同算子类型和配置。对于每个模型如（vgg），我们通过随机改变网络深度和宽度产生1000个变体。具体来说我们会对它的输出通道数在[0.2 × Cout , 1.8 × Cout]中随机抽取，核大小在{1, 3, 5, 7, 9}中抽取，以此改变宽度。在其中随机加入点卷积来改变深度。

​	最后，我们的数据集总共包含了10000个模型，它们都是由224x224x3大小的图像输入。如表中所示，我们的数据集包含了不同水平的flops和macs的范围。

特征提取：特征提取我们分为了两个部分，模型部分和硬件部分。模型部分的主要包括计算量、放存量、参数量、conv层数、fc层数、BN层数。硬件部分包含了静态信息和动态信息，静态信息包括cache size大小，动态信息包括访存命中和缺失，频率、带宽和cpu使用率。

性能建模：为了能够精确的对性能进行建模，我们采用机器学习的方法对所提取得到的特征进行建模，具体来说我们备选了不同创建且有效的模型，如linear、MLP、SVR、RF和XGBoost模型。通过每次从零添加新的特征到机器学习模型进行训练，直到训练得到的模型误差不下降为止。最后选择最后的模型对神经网络性能进行预测。



### implementation

我们已经在mindspore lite上实现了各种经典网络在对小米k40，华为p9，一加手机性能预测系统，同时，本系统也支持MNN，TFlite，NCNN移动推理引擎在设备上进行神经网络的性能预测。整个系统代码由3000余行python代码和cpp代码构成。它对10000个基准数据集进行了测试和特征提取，训练，并进行了有效的性能预测。

时延测量：我们的系统目前针对移动手机上的8核异构处理器如小米k40的晓龙888，华为p9的麒麟955，一加的天玑8100-MAX。分别适用不同的推理引擎框架，mindspore lite ，MNN，TFlite，NCNN。

​	我们建立了一个自动测量平台来测量延迟，首先我们需要将测量的模型文件有pc端发送到目标测量平台，并且收集返回推理的延迟，对于推理的延迟我们只考虑其推理一次的时延，与此同时测量得到我i们的硬件的动态信息。为了保证模型不受大小核调度的影响，我们的利用绑核的技术以及根据系统调度策略调整测试代码。

特征提取：

​	我们的特征提取包含硬件特征和模型特征两个部分，其中的硬件特征包括静态信息和动态信息，静态信息如cachesize直接由厂家信息得到，而动态信息需要我们在测试的时候，在测量时延的同时，还需要测量当前硬件的一些信息，这里我们arm的PMU（Performance Monitoring Unit）系统，它用于跟踪、计数系统内部的一些底层硬件事件，如与CPU有关的事件（执行指令数、捕获异常数、时钟周期数等）、与cache有关的事件（data/inst./L1/L2 cache访问次数，miss次数等）以及与TLB有关的事件等。这些事件反映了程序执行期的行为，从中我们可以提取出硬件系统动态变化的信息。

建模和预测

​	我们的目标是通过我们采集得到的特征，通过机器学习的方法，拟合出一个性能预测的模型，这里我们会在各种机器学习linear、MLP、SVR、RF和XGBoost中做出筛选，其评价标准就是模型的精确度和时延，一方面是我为了我们的模型更加的精确，一方面保障模型的轻量级，方便于后续的模型调优和加速。

#### Evaluation

In this section, we evaluate our system and baselines in various aspects to demonstrate the efficiency of our system.

评估的方式采用的MAE，MAEP，RMSE，5%error，10%error，20%error的方式，如图所示我们的不同模型在三种硬件平台上的指标。

### Discussion

### conclusion





### 参考目录

[1] mobilenet

[2]SqueezeNet

[3]ShuffleNet

[4]augur