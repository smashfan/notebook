# torch.optim.SGD参数学习率lr、动量momentum、权重衰减weight_decay的解析

### 简介

神经网络优化器，主要是为了优化我们的神经网络，使他在我们的训练过程中快起来，节省社交网络训练的时间。在pytorch中提供了torch.optim方法优化我们的神经网络，torch.optim是实现各种优化算法的包。最常用的方法都已经支持，接口很常规，所以以后也可以很容易地集成更复杂的方法。

### 如何使用optimizer

要使用[torch](https://so.csdn.net/so/search?q=torch&spm=1001.2101.3001.7020).optim，您必须构造一个optimizer对象。这个对象能保存当前的参数状态并且基于计算梯度更新参数。

构建一个优化器
要构造一个Optimizer，你必须给它一个包含参数（必须都是Variable对象）进行优化。然后，您可以指定optimizer的参数选项，比如学习率，权重衰减等。具体参考torch.optim中文文档

```
optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr = 0.0001)
```

### Stochastic Gradient Descent (SGD)

SGD是最基础的优化方法，普通的训练方法, 需要重复不断的把整套数据放入神经网络NN中训练, 这样消耗的计算资源会很大.当我们使用SGD会把数据拆分后再分批不断放入 NN 中计算. 每次使用批数据, 虽然不能反映整体数据的情况, 不过却很大程度上加速了 NN 的训练过程, 而且也不会丢失太多准确率.

### Momentum

Momentum 传统的参数 W 的更新是把原始的 W 累加上一个负的学习率(learning rate) 乘以校正值 (dx). 此方法比较曲折。

我们把这个人从平地上放到了一个斜坡上, 只要他往下坡的方向走一点点, 由于向下的惯性, 他不自觉地就一直往下走, 走的弯路也变少了. 这就是 Momentum 参数更新。


## learning rate

学习率较小时，收敛到极值的速度较慢。
学习率较大时，容易在搜索过程中发生震荡。

### weight decay

为了有效限制模型中的自由参数数量以避免过度拟合，可以调整成本函数。
一个简单的方法是通过在权重上引入零均值高斯先验值，这相当于将代价函数改变为E〜（w）= E（w）+λ2w2。
在实践中，这会惩罚较大的权重，并有效地限制模型中的自由度。
正则化参数λ决定了如何将原始成本E与大权重惩罚进行折衷。


### learning rate decay

在使用梯度下降法求解目标函数func(x) = x * x的极小值时，更新公式为x += v，其中每次x的更新量v为v = - dx * lr，dx为目标函数func(x)对x的一阶导数。可以想到，如果能够让lr随着迭代周期不断衰减变小，那么搜索时迈的步长就能不断减少以减缓震荡。学习率衰减因子由此诞生：
lri=lrstart∗1.0/(1.0+decay∗i)
lri=lrstart∗1.0/(1.0+decay∗i)
decay越小，学习率衰减地越慢，当decay = 0时，学习率保持不变。
decay越大，学习率衰减地越快，当decay = 1时，学习率衰减最快。
————————————————

### momentum

“冲量”这个概念源自于物理中的力学，表示力对时间的积累效应。
在普通的梯度下降法x+=v
中，每次x的更新量v为v=−dx∗lr，其中dx为目标函数func(x)对x的一阶导数，。
当使用冲量时，则把每次x的更新量v考虑为本次的梯度下降量−dx∗lr与上次x的更新量v乘上一个介于[0,1][0,1]的因子momentum的和，即
v=−dx∗lr+v∗momemtum
v=−dx∗lr+v∗momemtum
当本次梯度下降- dx * lr的方向与上次更新量v的方向相同时，上次的更新量能够对本次的搜索起到一个正向加速的作用。
当本次梯度下降- dx * lr的方向与上次更新量v的方向相反时，上次的更新量能够对本次的搜索起到一个减速的作用。

